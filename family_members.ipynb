{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Espacenet_search_result.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Title</th>\n",
       "      <th>Inventors</th>\n",
       "      <th>Applicants</th>\n",
       "      <th>Publication number</th>\n",
       "      <th>Earliest priority</th>\n",
       "      <th>IPC</th>\n",
       "      <th>CPC</th>\n",
       "      <th>Publication date</th>\n",
       "      <th>Earliest publication</th>\n",
       "      <th>Family number</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>first publication date</th>\n",
       "      <th>second publication date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Plug-in hybrid vehicle with fast energy storage</td>\n",
       "      <td>BENDER DONALD A [US] \\nDESHMANE ATUL [US] \\nME...</td>\n",
       "      <td>AFS TRINITY POWER CORP [US]</td>\n",
       "      <td>US2006250902A1</td>\n",
       "      <td>2005-05-05</td>\n",
       "      <td>H04B1/20</td>\n",
       "      <td>B60K6/20 (KR) \\nB60K6/28 (KR) \\nB60K6/30 (EP,U...</td>\n",
       "      <td>2006-11-09</td>\n",
       "      <td>2006-11-09</td>\n",
       "      <td>37397092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006-11-09</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Electric vehicle wireless charging system thro...</td>\n",
       "      <td>SON JEONG KI [KR]</td>\n",
       "      <td>SON JEONG KI [KR]</td>\n",
       "      <td>KR20230163874A</td>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>B60L53/12 \\nB60L53/38 \\nB60L53/54 \\nB60L53/66 ...</td>\n",
       "      <td>B60L53/12 (KR) \\nB60L53/38 (KR) \\nB60L53/54 (K...</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>89124565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Electric vehicle charging system using hydroge...</td>\n",
       "      <td>HONG SEONG HO</td>\n",
       "      <td>HOGREENAIR CO LTD [KR]</td>\n",
       "      <td>KR102511391B1</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>B60L3/00 \\nB60L53/16 \\nB60L53/51 \\nB60L53/54 \\...</td>\n",
       "      <td>Y02T10/70 (EP)</td>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>85796535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>BATTERY-CHARGING SYSTEM FOR AN ELECTRIC VEHICLE</td>\n",
       "      <td>CHUNG YON JONG [KR]</td>\n",
       "      <td>CHUNG YON JONG [KR]</td>\n",
       "      <td>WO2011019133A2 \\nWO2011019133A3</td>\n",
       "      <td>2009-08-13</td>\n",
       "      <td>B60L11/18 \\nB60W10/24 \\nH02J7/00</td>\n",
       "      <td>B60K6/46 (EP) \\nB60L15/2045 (EP) \\nB60L58/12 (...</td>\n",
       "      <td>2011-02-17 \\n2011-04-07</td>\n",
       "      <td>2010-03-25</td>\n",
       "      <td>42183715</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-02-17</td>\n",
       "      <td>2011-04-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ESS SYSTEM FOR CHARGING FUEL CELL ELECTRIC VEH...</td>\n",
       "      <td>MUN TAEEUN [KR] \\nCHAE HO BYUNG [KR] \\nSHIN JO...</td>\n",
       "      <td>SIGNET EV INC [KR] \\nSIGNET ENERGY [KR]</td>\n",
       "      <td>US2022140365A1</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>H01M16/00 \\nH01M8/04828 \\nH02J7/34</td>\n",
       "      <td>B60K15/03006 (KR) \\nB60L53/20 (KR) \\nB60L53/50...</td>\n",
       "      <td>2022-05-05</td>\n",
       "      <td>2021-03-17</td>\n",
       "      <td>75243546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-05-05</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   No                                              Title  \\\n",
       "0   1    Plug-in hybrid vehicle with fast energy storage   \n",
       "1   2  Electric vehicle wireless charging system thro...   \n",
       "2   3  Electric vehicle charging system using hydroge...   \n",
       "3   4    BATTERY-CHARGING SYSTEM FOR AN ELECTRIC VEHICLE   \n",
       "4   5  ESS SYSTEM FOR CHARGING FUEL CELL ELECTRIC VEH...   \n",
       "\n",
       "                                           Inventors  \\\n",
       "0  BENDER DONALD A [US] \\nDESHMANE ATUL [US] \\nME...   \n",
       "1                                  SON JEONG KI [KR]   \n",
       "2                                      HONG SEONG HO   \n",
       "3                                CHUNG YON JONG [KR]   \n",
       "4  MUN TAEEUN [KR] \\nCHAE HO BYUNG [KR] \\nSHIN JO...   \n",
       "\n",
       "                                Applicants               Publication number  \\\n",
       "0              AFS TRINITY POWER CORP [US]                   US2006250902A1   \n",
       "1                        SON JEONG KI [KR]                   KR20230163874A   \n",
       "2                   HOGREENAIR CO LTD [KR]                    KR102511391B1   \n",
       "3                      CHUNG YON JONG [KR]  WO2011019133A2 \\nWO2011019133A3   \n",
       "4  SIGNET EV INC [KR] \\nSIGNET ENERGY [KR]                   US2022140365A1   \n",
       "\n",
       "  Earliest priority                                                IPC  \\\n",
       "0        2005-05-05                                           H04B1/20   \n",
       "1        2022-05-24  B60L53/12 \\nB60L53/38 \\nB60L53/54 \\nB60L53/66 ...   \n",
       "2        2022-09-20  B60L3/00 \\nB60L53/16 \\nB60L53/51 \\nB60L53/54 \\...   \n",
       "3        2009-08-13                   B60L11/18 \\nB60W10/24 \\nH02J7/00   \n",
       "4        2020-11-02                 H01M16/00 \\nH01M8/04828 \\nH02J7/34   \n",
       "\n",
       "                                                 CPC         Publication date  \\\n",
       "0  B60K6/20 (KR) \\nB60K6/28 (KR) \\nB60K6/30 (EP,U...               2006-11-09   \n",
       "1  B60L53/12 (KR) \\nB60L53/38 (KR) \\nB60L53/54 (K...               2023-12-01   \n",
       "2                                     Y02T10/70 (EP)               2023-03-20   \n",
       "3  B60K6/46 (EP) \\nB60L15/2045 (EP) \\nB60L58/12 (...  2011-02-17 \\n2011-04-07   \n",
       "4  B60K15/03006 (KR) \\nB60L53/20 (KR) \\nB60L53/50...               2022-05-05   \n",
       "\n",
       "  Earliest publication  Family number  Unnamed: 11 first publication date  \\\n",
       "0           2006-11-09       37397092          NaN             2006-11-09   \n",
       "1           2023-12-01       89124565          NaN             2023-12-01   \n",
       "2           2023-03-20       85796535          NaN             2023-03-20   \n",
       "3           2010-03-25       42183715          NaN             2011-02-17   \n",
       "4           2021-03-17       75243546          NaN             2022-05-05   \n",
       "\n",
       "  second publication date  \n",
       "0                    None  \n",
       "1                    None  \n",
       "2                    None  \n",
       "3              2011-04-07  \n",
       "4                    None  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['first publication date','second publication date']] = df['Publication date'].str.split(' ' , n=1 , expand= True)\n",
    "df['second publication date'] = df['second publication date'].str.strip('\\n')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_more_options_button(self):\n",
    "    \"\"\"Find and click the 'More Options' button.\"\"\"\n",
    "    try:\n",
    "        # Debug: Print all buttons on the page\n",
    "        buttons = self.driver.find_elements(By.TAG_NAME, \"button\")\n",
    "        print(f\"Found {len(buttons)} buttons on the page:\")\n",
    "        for button in buttons:\n",
    "            print(f\"Button text: {button.text}, Class: {button.get_attribute('class')}\")\n",
    "\n",
    "        # Wait for the \"More Options\" button to be clickable\n",
    "        more_options_button = WebDriverWait(self.driver, 20).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.prod-jss36.prod-jss27\"))\n",
    "        )\n",
    "\n",
    "        # Click the \"More Options\" button\n",
    "        more_options_button.click()\n",
    "        print(\"Clicked 'More Options' button\")\n",
    "\n",
    "        # Debug: Print all dropdown menus on the page\n",
    "        dropdown_menus = self.driver.find_elements(By.CSS_SELECTOR, \"div.prod-jss546\")\n",
    "        print(f\"Found {len(dropdown_menus)} dropdown menus on the page:\")\n",
    "        for menu in dropdown_menus:\n",
    "            print(f\"Dropdown menu HTML: {menu.get_attribute('innerHTML')}\")\n",
    "\n",
    "        # Wait for the dropdown menu to appear\n",
    "        WebDriverWait(self.driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.prod-jss546\"))\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error clicking 'More Options' button: {e}\")\n",
    "\n",
    "def click_download_button(self):\n",
    "    \"\"\"Find and click the 'Download' button in the dropdown menu.\"\"\"\n",
    "    try:\n",
    "        # Debug: Print all download buttons on the page\n",
    "        download_buttons = self.driver.find_elements(By.CSS_SELECTOR, \"section.prod-jss36\")\n",
    "        print(f\"Found {len(download_buttons)} download buttons on the page:\")\n",
    "        for button in download_buttons:\n",
    "            print(f\"Download button text: {button.text}, Class: {button.get_attribute('class')}\")\n",
    "\n",
    "        # Wait for the \"Download\" button to be clickable\n",
    "        download_button = WebDriverWait(self.driver, 20).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"section.prod-jss36.prod-jss700\"))\n",
    "        )\n",
    "\n",
    "        # Click the \"Download\" button\n",
    "        download_button.click()\n",
    "        print(\"Clicked 'Download' button\")\n",
    "\n",
    "        # Wait for the file to be downloaded\n",
    "        time.sleep(10)  # Adjust this delay based on your network speed\n",
    "\n",
    "        # Find the downloaded file\n",
    "        downloaded_file = os.path.join(self.download_dir, \"search_results.csv\")\n",
    "        if os.path.exists(downloaded_file):\n",
    "            print(f\"File downloaded successfully: {downloaded_file}\")\n",
    "\n",
    "            # Read the CSV file\n",
    "            with open(downloaded_file, mode='r', encoding='utf-8') as file:\n",
    "                csv_reader = csv.DictReader(file)\n",
    "                for row in csv_reader:\n",
    "                    print(row)\n",
    "        else:\n",
    "            print(\"File not found. Check the download directory.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error clicking 'Download' button: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_more_options_button(self):\n",
    "    \"\"\"Find and click the 'Menu' button.\"\"\"\n",
    "    try:\n",
    "        # Wait for the \"Menu\" button to be clickable\n",
    "        more_options_button = WebDriverWait(self.driver, 20).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[contains(@aria-label, 'Menu') or contains(text(), 'Menu')]\"))\n",
    "        )\n",
    "\n",
    "        # Click the \"Menu\" button\n",
    "        more_options_button.click()\n",
    "        print(\"Clicked 'Menu' button\")\n",
    "\n",
    "        # Add a delay to wait for the dropdown to appear\n",
    "        time.sleep(2)  # Adjust this delay as needed\n",
    "\n",
    "        # Debug: Print the HTML of the dropdown menu\n",
    "        dropdown_menus = self.driver.find_elements(By.CSS_SELECTOR, \"div.prod-jss546\")\n",
    "        print(f\"Found {len(dropdown_menus)} dropdown menus on the page:\")\n",
    "        for menu in dropdown_menus:\n",
    "            print(f\"Dropdown menu HTML: {menu.get_attribute('innerHTML')}\")\n",
    "\n",
    "        # Wait for the dropdown menu to appear\n",
    "        WebDriverWait(self.driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.prod-jss546\"))\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error clicking 'Menu' button: {e}\")\n",
    "\n",
    "def click_download_button(self):\n",
    "    \"\"\"Find and click the 'Télécharger' button in the dropdown menu.\"\"\"\n",
    "    try:\n",
    "        # Wait for the \"Télécharger\" button to be clickable\n",
    "        download_button = WebDriverWait(self.driver, 20).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//section[contains(text(), 'Télécharger')]\"))\n",
    "        )\n",
    "\n",
    "        # Click the \"Télécharger\" button\n",
    "        download_button.click()\n",
    "        print(\"Clicked 'Télécharger' button\")\n",
    "\n",
    "        # Wait for the file to be downloaded\n",
    "        time.sleep(10)  # Adjust this delay based on your network speed\n",
    "\n",
    "        # Find the downloaded file\n",
    "        downloaded_file = os.path.join(self.download_dir, \"search_results.csv\")\n",
    "        if os.path.exists(downloaded_file):\n",
    "            print(f\"File downloaded successfully: {downloaded_file}\")\n",
    "\n",
    "            # Read the CSV file\n",
    "            with open(downloaded_file, mode='r', encoding='utf-8') as file:\n",
    "                csv_reader = csv.DictReader(file)\n",
    "                for row in csv_reader:\n",
    "                    print(row)\n",
    "        else:\n",
    "            print(\"File not found. Check the download directory.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error clicking 'Télécharger' button: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing the family number column \n",
    "import time\n",
    "import random\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "class EspacenetScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        \"\"\"Initialize the scraper with configurable options.\"\"\"\n",
    "        options = uc.ChromeOptions()\n",
    "        if headless:\n",
    "            options.add_argument('--headless')  # Run in headless mode\n",
    "\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        self.driver = uc.Chrome(options=options)\n",
    "        self.driver.set_page_load_timeout(30)\n",
    "        self.driver.set_window_size(1920, 1080)\n",
    "\n",
    "    def add_random_delay(self, min_seconds=1, max_seconds=3):\n",
    "        \"\"\"Add a random delay to mimic human behavior.\"\"\"\n",
    "        time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "    def get_page_html(self, url):\n",
    "        \"\"\"Navigate to the given URL and return the page HTML.\"\"\"\n",
    "        try:\n",
    "            print(f\"Navigating to: {url}\")\n",
    "            self.driver.get(url)\n",
    "\n",
    "            # Wait for the page to load completely\n",
    "            WebDriverWait(self.driver, 20).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "\n",
    "            # Add a random delay to mimic human behavior\n",
    "            self.add_random_delay(3, 5)\n",
    "\n",
    "            # Return the page HTML\n",
    "            return self.driver.page_source\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\"Timed out waiting for the page to load.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse_html(self, html):\n",
    "        \"\"\"Parse the HTML and extract all span elements inside the 'Published as' content.\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Look for the element containing \"Publié en tant que\" or \"Published as\"\n",
    "        published_as_element = soup.find(lambda tag: tag.name == \"h5\" and (\"Publié en tant que\" in tag.text or \"Published as\" in tag.text))\n",
    "        \n",
    "        if published_as_element:\n",
    "            # Get the next sibling span that contains the relevant content\n",
    "            content_element = published_as_element.find_next_sibling(\"span\")\n",
    "            if content_element:\n",
    "                # Extract all span elements within the content\n",
    "                spans = content_element.find_all('span')\n",
    "                return [span.get_text(strip=True) for span in spans]\n",
    "        return []\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the browser when done.\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "# Example Usage:\n",
    "if __name__ == '__main__':\n",
    "    # Initialize the scraper\n",
    "    scraper = EspacenetScraper(headless=False)  # Set headless to False to see the browser in action\n",
    "\n",
    "    # Create a DataFrame with family numbers and publication numbers\n",
    "    #data = {\n",
    "     #   'family_number': ['087517563', 'another_family_number'],  # Replace with actual family numbers\n",
    "      #  'publication_number': ['GB2631304A', 'another_publication_number']  # Replace with actual publication numbers\n",
    "  #  }\n",
    "   # df = pd.DataFrame(data)\n",
    "\n",
    "    # Create a new column for family members\n",
    "    df['family_members'] = None\n",
    "\n",
    "    try:\n",
    "        for index, row in df.iterrows():\n",
    "            # Construct the URL using family number and publication number\n",
    "            url = f\"https://worldwide.espacenet.com/patent/search/family/{row['Family number']}/publication/{row['first publication number']}?q=hydrogen%20battery\"\n",
    "\n",
    "            # Get the page HTML\n",
    "            html = scraper.get_page_html(url)\n",
    "            if html:\n",
    "                print(f\"Page HTML retrieved successfully for {row['first publication number']}.\")\n",
    "                # Parse the HTML to find the relevant content\n",
    "                family_members = scraper.parse_html(html)\n",
    "                df.at[index, 'family_members'] = family_members  # Store the result in the DataFrame\n",
    "            else:\n",
    "                print(f\"Failed to retrieve the page HTML for {row['first publication number']}.\")\n",
    "\n",
    "    finally:\n",
    "        # Close the browser\n",
    "        scraper.close()\n",
    "        print(\"Scraper closed.\")\n",
    "\n",
    "    # Display the DataFrame with\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paralell processing family members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n",
      "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 165\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# Assuming 'df' is your existing DataFrame\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# Load your DataFrame here or use the existing one\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Perform parallel extraction\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     updated_df \u001b[38;5;241m=\u001b[39m \u001b[43mparallel_extract_family_members\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28mprint\u001b[39m(updated_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst publication number\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfamily_members\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[3], line 149\u001b[0m, in \u001b[0;36mparallel_extract_family_members\u001b[1;34m(df, max_workers)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Use ThreadPoolExecutor for I/O-bound tasks like web scraping\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;66;03m# Submit tasks for each row\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m     futures \u001b[38;5;241m=\u001b[39m [executor\u001b[38;5;241m.\u001b[39msubmit(\u001b[43mPatentFamilyScraper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mprocess_patent, row) \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df_copy\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# Collect results\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mas_completed(futures):\n",
      "Cell \u001b[1;32mIn[3], line 28\u001b[0m, in \u001b[0;36mPatentFamilyScraper.__init__\u001b[1;34m(self, headless)\u001b[0m\n\u001b[0;32m     25\u001b[0m options\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--disable-extensions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver \u001b[38;5;241m=\u001b[39m \u001b[43muc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_subprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mversion_main\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_welcome\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39mset_page_load_timeout(\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39mset_window_size(\u001b[38;5;241m1920\u001b[39m, \u001b[38;5;241m1080\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\undetected_chromedriver\\__init__.py:466\u001b[0m, in \u001b[0;36mChrome.__init__\u001b[1;34m(self, options, user_data_dir, driver_executable_path, browser_executable_path, port, enable_cdp_events, desired_capabilities, advanced_elements, keep_alive, log_level, headless, version_main, patcher_force_close, suppress_welcome, use_subprocess, debug, no_sandbox, user_multi_procs, **kw)\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbrowser_pid \u001b[38;5;241m=\u001b[39m browser\u001b[38;5;241m.\u001b[39mpid\n\u001b[0;32m    462\u001b[0m service \u001b[38;5;241m=\u001b[39m selenium\u001b[38;5;241m.\u001b[39mwebdriver\u001b[38;5;241m.\u001b[39mchromium\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mChromiumService(\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatcher\u001b[38;5;241m.\u001b[39mexecutable_path\n\u001b[0;32m    464\u001b[0m )\n\u001b[1;32m--> 466\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChrome\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreactor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enable_cdp_events:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[0;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:66\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[1;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     57\u001b[0m executor \u001b[38;5;241m=\u001b[39m ChromiumRemoteConnection(\n\u001b[0;32m     58\u001b[0m     remote_server_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mservice_url,\n\u001b[0;32m     59\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mbrowser_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m     ignore_proxy\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m_ignore_local_proxy,\n\u001b[0;32m     63\u001b[0m )\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquit()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:250\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, command_executor, keep_alive, file_detector, options, locator_converter, web_element_cls, client_config)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_authenticator_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_client()\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcapabilities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fedcm \u001b[38;5;241m=\u001b[39m FedCM(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_websocket_connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\undetected_chromedriver\\__init__.py:724\u001b[0m, in \u001b[0;36mChrome.start_session\u001b[1;34m(self, capabilities, browser_profile)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m capabilities:\n\u001b[0;32m    723\u001b[0m     capabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mto_capabilities()\n\u001b[1;32m--> 724\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mselenium\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchrome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWebDriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_session\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapabilities\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:342\u001b[0m, in \u001b[0;36mWebDriver.start_session\u001b[1;34m(self, capabilities)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new session with the desired capabilities.\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \n\u001b[0;32m    335\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03m    - A capabilities dict to start the session with.\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    341\u001b[0m caps \u001b[38;5;241m=\u001b[39m _create_caps(capabilities)\n\u001b[1;32m--> 342\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNEW_SESSION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaps\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaps \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapabilities\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:427\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m    425\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m--> 427\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:404\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    402\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[0;32m    403\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[1;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:428\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    425\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 428\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\_request_methods.py:144\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[0;32m    137\u001b[0m         method,\n\u001b[0;32m    138\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[0;32m    142\u001b[0m     )\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\_request_methods.py:279\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    275\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[0;32m    277\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\poolmanager.py:444\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    442\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2544.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:1430\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1430\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1431\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1432\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2544.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2544.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2544.0_x64__qbz5n2kfra8p0\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import os\n",
    "\n",
    "class PatentFamilyScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        \"\"\"Initialize the scraper with enhanced compatibility options.\"\"\"\n",
    "        \n",
    "        options = uc.ChromeOptions()\n",
    "        \n",
    "        if headless:\n",
    "            options.add_argument('--headless')\n",
    "        \n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_argument('--disable-extensions')\n",
    "        \n",
    "        try:\n",
    "            self.driver = uc.Chrome(\n",
    "                options=options, \n",
    "                use_subprocess=True,  \n",
    "                version_main=None,    \n",
    "                suppress_welcome=True,\n",
    "                debug=False\n",
    "            )\n",
    "            \n",
    "            self.driver.set_page_load_timeout(30)\n",
    "            self.driver.set_window_size(1920, 1080)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize ChromeDriver: {e}\")\n",
    "            print(\"Trying alternative initialization method...\")\n",
    "            \n",
    "            # Alternative initialization method\n",
    "            self.driver = uc.Chrome(\n",
    "                options=options,\n",
    "                driver_executable_path=None  \n",
    "            )\n",
    "\n",
    "    def add_random_delay(self, min_seconds=1, max_seconds=3):\n",
    "        \"\"\"Add a random delay to mimic human behavior.\"\"\"\n",
    "        time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "    def get_page_html(self, url):\n",
    "        \"\"\"Navigate to the given URL and return the page HTML.\"\"\"\n",
    "        try:\n",
    "            print(f\"Navigating to: {url}\")\n",
    "            self.driver.get(url)\n",
    "\n",
    "            WebDriverWait(self.driver, 20).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "\n",
    "            self.add_random_delay(3, 5)\n",
    "\n",
    "            return self.driver.page_source\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\"Timed out waiting for the page to load.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse_html(self, html):\n",
    "        \"\"\"Parse the HTML and extract all span elements inside the 'Published as' content.\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Look for the element containing \"Publié en tant que\" or \"Published as\"\n",
    "        published_as_element = soup.find(lambda tag: tag.name == \"h5\" and (\"Publié en tant que\" in tag.text or \"Published as\" in tag.text))\n",
    "        \n",
    "        if published_as_element:\n",
    "            # Get the next sibling span that contains the relevant content\n",
    "            content_element = published_as_element.find_next_sibling(\"span\")\n",
    "            if content_element:\n",
    "                # Extract all span elements within the content\n",
    "                spans = content_element.find_all('span')\n",
    "                return [span.get_text(strip=True) for span in spans]\n",
    "        return []\n",
    "\n",
    "    def process_patent(self, row):\n",
    "        \"\"\"Process a single patent row to extract family members.\"\"\"\n",
    "        try:\n",
    "            # Create a new driver for each thread to avoid conflicts\n",
    "            scraper = PatentFamilyScraper(headless=True)\n",
    "            \n",
    "            url = f\"https://worldwide.espacenet.com/patent/search/family/{row['Family number']}/publication/{row['first publication number']}?q=hydrogen%20battery\"\n",
    "\n",
    "            html = scraper.get_page_html(url)\n",
    "            \n",
    "            if html:\n",
    "                print(f\"Page HTML retrieved successfully for {row['first publication number']}.\")\n",
    "                \n",
    "                family_members = scraper.parse_html(html)\n",
    "                \n",
    "                # Close the driver for this thread\n",
    "                scraper.close()\n",
    "                \n",
    "                return {\n",
    "                    'index': row.name,  # Use the DataFrame index\n",
    "                    'family_members': family_members\n",
    "                }\n",
    "            else:\n",
    "                print(f\"Failed to retrieve the page HTML for {row['first publication number']}.\")\n",
    "                scraper.close()\n",
    "                return {\n",
    "                    'index': row.name,\n",
    "                    'family_members': None\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['first publication number']}: {e}\")\n",
    "            return {\n",
    "                'index': row.name,\n",
    "                'family_members': None\n",
    "            }\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the browser when done.\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "def parallel_extract_family_members(df, max_workers=5):\n",
    "    \"\"\"\n",
    "    Extract family members in parallel using multiple worker threads.\n",
    "    \n",
    "    :param df: DataFrame containing patent information\n",
    "    :param max_workers: Maximum number of concurrent threads (default 5)\n",
    "    :return: Updated DataFrame with family members\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Initialize the family_members column if not exists\n",
    "    if 'family_members' not in df_copy.columns:\n",
    "        df_copy['family_members'] = None\n",
    "    \n",
    "    # Use ThreadPoolExecutor for I/O-bound tasks like web scraping\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit tasks for each row\n",
    "        futures = [executor.submit(PatentFamilyScraper().process_patent, row) for _, row in df_copy.iterrows()]\n",
    "        \n",
    "        # Collect results\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result['family_members'] is not None:\n",
    "                df_copy.at[result['index'], 'family_members'] = result['family_members']\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    # Assuming 'df' is your existing DataFrame\n",
    "    # Load your DataFrame here or use the existing one\n",
    "    \n",
    "    # Perform parallel extraction\n",
    "    updated_df = parallel_extract_family_members(df, max_workers=os.cpu_count() or 5)\n",
    "    \n",
    "    # Display the results\n",
    "    print(updated_df[['first publication number', 'family_members']].head())\n",
    "    \n",
    "    # Optional: Save to CSV\n",
    "    #updated_df.to_csv('patent_family_members.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import os\n",
    "import subprocess\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import re\n",
    "\n",
    "def get_chrome_version():\n",
    "    \"\"\"\n",
    "    Attempt to detect Chrome version across different operating systems.\n",
    "    \n",
    "    Returns:\n",
    "    int: Major version number of Chrome, or None if detection fails\n",
    "    \"\"\"\n",
    "    chrome_versions = {\n",
    "        'win32': [\n",
    "            r'C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe',\n",
    "            r'C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe'\n",
    "        ],\n",
    "        'darwin': [\n",
    "            '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome',\n",
    "            '/usr/bin/google-chrome'\n",
    "        ],\n",
    "        'linux': [\n",
    "            '/usr/bin/google-chrome',\n",
    "            '/usr/bin/chromium-browser',\n",
    "            '/usr/bin/chromium'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    import sys\n",
    "    platform = sys.platform\n",
    "\n",
    "    version_pattern = re.compile(r'(\\d+)\\.')\n",
    "\n",
    "    # Try different potential Chrome executable paths\n",
    "    for path in chrome_versions.get(platform, []):\n",
    "        try:\n",
    "            if os.path.exists(path):\n",
    "                # Different commands for different platforms\n",
    "                if platform == 'win32':\n",
    "                    version_output = subprocess.check_output([path, '--version'], universal_newlines=True)\n",
    "                else:\n",
    "                    version_output = subprocess.check_output([path, '--version'], universal_newlines=True)\n",
    "                \n",
    "                match = version_pattern.search(version_output)\n",
    "                if match:\n",
    "                    return int(match.group(1))\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "            continue\n",
    "\n",
    "    # Fallback method for some systems\n",
    "    try:\n",
    "        # Try using Chrome itself to get version\n",
    "        version_output = subprocess.check_output(['chrome', '--version'], universal_newlines=True)\n",
    "        match = version_pattern.search(version_output)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        pass\n",
    "\n",
    "    # If all methods fail, return a default or None\n",
    "    print(\"Warning: Could not automatically detect Chrome version. Defaulting to version 108.\")\n",
    "    return 108  # Default fallback version\n",
    "\n",
    "class PatentFamilyScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        \"\"\"Initialize the scraper with enhanced compatibility options.\"\"\"\n",
    "        \n",
    "        # Detect Chrome version\n",
    "        chrome_version = get_chrome_version()\n",
    "        \n",
    "        options = uc.ChromeOptions()\n",
    "        \n",
    "        if headless:\n",
    "            options.add_argument('--headless')\n",
    "        \n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_argument('--disable-extensions')\n",
    "        \n",
    "        try:\n",
    "            self.driver = uc.Chrome(\n",
    "                options=options, \n",
    "                use_subprocess=True,  \n",
    "                version_main=chrome_version,  # Use detected version\n",
    "                suppress_welcome=True,\n",
    "                debug=False\n",
    "            )\n",
    "            \n",
    "            self.driver.set_page_load_timeout(30)\n",
    "            self.driver.set_window_size(1920, 1080)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize ChromeDriver with detected version {chrome_version}: {e}\")\n",
    "            print(\"Trying alternative initialization method...\")\n",
    "            \n",
    "            # Alternative initialization method with explicit version\n",
    "            self.driver = uc.Chrome(\n",
    "                options=options,\n",
    "                version_main=108,  # Explicit fallback version\n",
    "                driver_executable_path=None  \n",
    "            )\n",
    "\n",
    "    # ... [rest of the class remains the same as in the previous implementation]\n",
    "\n",
    "def parallel_extract_family_members(df, max_workers=None):\n",
    "    \"\"\"\n",
    "    Extract family members in parallel using multiple worker threads.\n",
    "    \n",
    "    :param df: DataFrame containing patent information\n",
    "    :param max_workers: Maximum number of concurrent threads (default to CPU count)\n",
    "    :return: Updated DataFrame with family members\n",
    "    \"\"\"\n",
    "    # Use CPU count if not specified, with a minimum of 1 and maximum of 10\n",
    "    if max_workers is None:\n",
    "        max_workers = min(max(1, os.cpu_count() or 1), 10)\n",
    "    \n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Initialize the family_members column if not exists\n",
    "    if 'family_members' not in df_copy.columns:\n",
    "        df_copy['family_members'] = None\n",
    "    \n",
    "    # Use ThreadPoolExecutor for I/O-bound tasks like web scraping\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit tasks for each row\n",
    "        futures = [executor.submit(PatentFamilyScraper().process_patent, row) for _, row in df_copy.iterrows()]\n",
    "        \n",
    "        # Collect results\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result['family_members'] is not None:\n",
    "                df_copy.at[result['index'], 'family_members'] = result['family_members']\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    # Load your DataFrame here\n",
    "    # df = pd.read_csv('your_patent_data.csv')\n",
    "    \n",
    "    # Perform parallel extraction\n",
    "    updated_df = parallel_extract_family_members(df)\n",
    "    \n",
    "    # Display the results\n",
    "    print(updated_df[['first publication number', 'family_members']].head())\n",
    "    \n",
    "    # Optional: Save to CSV\n",
    "    updated_df.to_csv('patent_family_members.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import os\n",
    "import subprocess\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import re\n",
    "\n",
    "def get_chrome_version():\n",
    "    \"\"\"\n",
    "    Attempt to detect Chrome version across different operating systems.\n",
    "    \n",
    "    Returns:\n",
    "    int: Major version number of Chrome, or None if detection fails\n",
    "    \"\"\"\n",
    "    chrome_versions = {\n",
    "        'win32': [\n",
    "            r'C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe',\n",
    "            r'C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe'\n",
    "        ],\n",
    "        'darwin': [\n",
    "            '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome',\n",
    "            '/usr/bin/google-chrome'\n",
    "        ],\n",
    "        'linux': [\n",
    "            '/usr/bin/google-chrome',\n",
    "            '/usr/bin/chromium-browser',\n",
    "            '/usr/bin/chromium'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    import sys\n",
    "    platform = sys.platform\n",
    "\n",
    "    version_pattern = re.compile(r'(\\d+)\\.')\n",
    "\n",
    "    # Try different potential Chrome executable paths\n",
    "    for path in chrome_versions.get(platform, []):\n",
    "        try:\n",
    "            if os.path.exists(path):\n",
    "                # Different commands for different platforms\n",
    "                if platform == 'win32':\n",
    "                    version_output = subprocess.check_output([path, '--version'], universal_newlines=True)\n",
    "                else:\n",
    "                    version_output = subprocess.check_output([path, '--version'], universal_newlines=True)\n",
    "                \n",
    "                match = version_pattern.search(version_output)\n",
    "                if match:\n",
    "                    return int(match.group(1))\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "            continue\n",
    "\n",
    "    # Fallback method for some systems\n",
    "    try:\n",
    "        # Try using Chrome itself to get version\n",
    "        version_output = subprocess.check_output(['chrome', '--version'], universal_newlines=True)\n",
    "        match = version_pattern.search(version_output)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        pass\n",
    "\n",
    "    # If all methods fail, return a default or None\n",
    "    print(\"Warning: Could not automatically detect Chrome version. Defaulting to version 108.\")\n",
    "    return 108  # Default fallback version\n",
    "\n",
    "class PatentFamilyScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        \"\"\"Initialize the scraper with enhanced compatibility options.\"\"\"\n",
    "        \n",
    "        # Detect Chrome version\n",
    "        chrome_version = get_chrome_version()\n",
    "        \n",
    "        options = uc.ChromeOptions()\n",
    "        \n",
    "        if headless:\n",
    "            options.add_argument('--headless')\n",
    "        \n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_argument('--disable-extensions')\n",
    "        \n",
    "        try:\n",
    "            self.driver = uc.Chrome(\n",
    "                options=options, \n",
    "                use_subprocess=True,  \n",
    "                version_main=chrome_version,  # Use detected version\n",
    "                suppress_welcome=True,\n",
    "                debug=False\n",
    "            )\n",
    "            \n",
    "            self.driver.set_page_load_timeout(30)\n",
    "            self.driver.set_window_size(1920, 1080)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize ChromeDriver with detected version {chrome_version}: {e}\")\n",
    "            print(\"Trying alternative initialization method...\")\n",
    "            \n",
    "            # Alternative initialization method with explicit version\n",
    "            self.driver = uc.Chrome(\n",
    "                options=options,\n",
    "                version_main=108,  # Explicit fallback version\n",
    "                driver_executable_path=None  \n",
    "            )\n",
    "\n",
    "    def add_random_delay(self, min_seconds=1, max_seconds=3):\n",
    "        \"\"\"Add a random delay to mimic human behavior.\"\"\"\n",
    "        time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "    def get_page_html(self, url):\n",
    "        \"\"\"\n",
    "        Navigate to the given URL and return the page HTML with robust error handling.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The URL to navigate to\n",
    "        \n",
    "        Returns:\n",
    "            str or None: Page source HTML if successful, None otherwise\n",
    "        \"\"\"\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"Attempt {attempt + 1}: Navigating to: {url}\")\n",
    "                \n",
    "                # Clear any existing cookies or local storage\n",
    "                self.driver.delete_all_cookies()\n",
    "                \n",
    "                # Navigate to the URL\n",
    "                self.driver.get(url)\n",
    "\n",
    "                # Wait for body element to ensure page is loaded\n",
    "                WebDriverWait(self.driver, 30).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "                )\n",
    "\n",
    "                # Add random delay to mimic human behavior\n",
    "                self.add_random_delay(3, 5)\n",
    "\n",
    "                # Return the page source\n",
    "                return self.driver.page_source\n",
    "\n",
    "            except TimeoutException:\n",
    "                print(f\"Timeout on attempt {attempt + 1}. Retrying...\")\n",
    "                continue\n",
    "            \n",
    "            except WebDriverException as e:\n",
    "                print(f\"WebDriver error on attempt {attempt + 1}: {e}\")\n",
    "                \n",
    "                # Add a longer delay between retries\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error on attempt {attempt + 1}: {e}\")\n",
    "                break\n",
    "\n",
    "        print(f\"Failed to retrieve HTML for {url} after {max_retries} attempts.\")\n",
    "        return None\n",
    "\n",
    "    def parse_html(self, html):\n",
    "        \"\"\"\n",
    "        Parse the HTML and extract family member information.\n",
    "        \n",
    "        Args:\n",
    "            html (str): HTML page source to parse\n",
    "        \n",
    "        Returns:\n",
    "            list: List of extracted family member details\n",
    "        \"\"\"\n",
    "        if not html:\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Look for the element containing \"Publié en tant que\" or \"Published as\"\n",
    "        published_as_element = soup.find(\n",
    "            lambda tag: tag.name == \"h5\" and \n",
    "            (\"Publié en tant que\" in tag.text or \"Published as\" in tag.text)\n",
    "        )\n",
    "        \n",
    "        if published_as_element:\n",
    "            # Get the next sibling span that contains the relevant content\n",
    "            content_element = published_as_element.find_next_sibling(\"span\")\n",
    "            if content_element:\n",
    "                # Extract all span elements within the content\n",
    "                spans = content_element.find_all('span')\n",
    "                return [span.get_text(strip=True) for span in spans]\n",
    "        \n",
    "        return []\n",
    "\n",
    "    def process_patent(self, row):\n",
    "        \"\"\"\n",
    "        Process a single patent row to extract family members.\n",
    "        \n",
    "        Args:\n",
    "            row (pandas.Series): Row from the patent DataFrame\n",
    "        \n",
    "        Returns:\n",
    "            dict: Processed patent information\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Construct URL for patent family\n",
    "            url = f\"https://worldwide.espacenet.com/patent/search/family/{row['Family number']}/publication/{row['first publication number']}?q=hydrogen%20battery\"\n",
    "\n",
    "            # Get page HTML\n",
    "            html = self.get_page_html(url)\n",
    "            \n",
    "            if html:\n",
    "                print(f\"Page HTML retrieved successfully for {row['first publication number']}.\")\n",
    "                \n",
    "                # Parse HTML to extract family members\n",
    "                family_members = self.parse_html(html)\n",
    "                \n",
    "                return {\n",
    "                    'index': row.name,  # Use the DataFrame index\n",
    "                    'family_members': family_members\n",
    "                }\n",
    "            else:\n",
    "                print(f\"Failed to retrieve the page HTML for {row['first publication number']}.\")\n",
    "                return {\n",
    "                    'index': row.name,\n",
    "                    'family_members': None\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['first publication number']}: {e}\")\n",
    "            return {\n",
    "                'index': row.name,\n",
    "                'family_members': None\n",
    "            }\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the browser when done.\"\"\"\n",
    "        if self.driver:\n",
    "            try:\n",
    "                self.driver.quit()\n",
    "            except Exception as e:\n",
    "                print(f\"Error closing driver: {e}\")\n",
    "\n",
    "def parallel_extract_family_members(df, max_workers=None):\n",
    "    \"\"\"\n",
    "    Extract family members in parallel using multiple worker threads.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing patent information\n",
    "        max_workers (int, optional): Maximum number of concurrent threads\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Updated DataFrame with family members\n",
    "    \"\"\"\n",
    "    # Use CPU count if not specified, with a minimum of 1 and maximum of 10\n",
    "    if max_workers is None:\n",
    "        max_workers = min(max(1, os.cpu_count() or 1), 10)\n",
    "    \n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Initialize the family_members column if not exists\n",
    "    if 'family_members' not in df_copy.columns:\n",
    "        df_copy['family_members'] = None\n",
    "    \n",
    "    # Create a scraper instance for getting the base URL setup\n",
    "    base_scraper = PatentFamilyScraper()\n",
    "    \n",
    "    try:\n",
    "        # Use ThreadPoolExecutor for I/O-bound tasks like web scraping\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit tasks for each row\n",
    "            futures = [executor.submit(base_scraper.process_patent, row) for _, row in df_copy.iterrows()]\n",
    "            \n",
    "            # Collect results\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result['family_members'] is not None:\n",
    "                    df_copy.at[result['index'], 'family_members'] = result['family_members']\n",
    "    \n",
    "    finally:\n",
    "        # Ensure the base scraper is closed\n",
    "        base_scraper.close()\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    # Load your DataFrame here\n",
    "    # df = pd.read_csv('your_patent_data.csv')\n",
    "    \n",
    "    # Perform parallel extraction\n",
    "    updated_df = parallel_extract_family_members(df)\n",
    "    \n",
    "    # Display the results\n",
    "    print(updated_df[['first publication number', 'family_members']].head())\n",
    "    \n",
    "    # Optional: Save to CSV\n",
    "    updated_df.to_csv('patent_family_members.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import os\n",
    "import subprocess\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import re\n",
    "\n",
    "def get_chrome_version():\n",
    "    \"\"\"\n",
    "    Attempt to detect Chrome version across different operating systems.\n",
    "    \n",
    "    Returns:\n",
    "    int: Major version number of Chrome, or None if detection fails\n",
    "    \"\"\"\n",
    "    chrome_versions = {\n",
    "        'win32': [\n",
    "            r'C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe',\n",
    "            r'C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe'\n",
    "        ],\n",
    "        'darwin': [\n",
    "            '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome',\n",
    "            '/usr/bin/google-chrome'\n",
    "        ],\n",
    "        'linux': [\n",
    "            '/usr/bin/google-chrome',\n",
    "            '/usr/bin/chromium-browser',\n",
    "            '/usr/bin/chromium'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    import sys\n",
    "    platform = sys.platform\n",
    "\n",
    "    version_pattern = re.compile(r'(\\d+)\\.')\n",
    "\n",
    "    # Try different potential Chrome executable paths\n",
    "    for path in chrome_versions.get(platform, []):\n",
    "        try:\n",
    "            if os.path.exists(path):\n",
    "                # Different commands for different platforms\n",
    "                if platform == 'win32':\n",
    "                    version_output = subprocess.check_output([path, '--version'], universal_newlines=True)\n",
    "                else:\n",
    "                    version_output = subprocess.check_output([path, '--version'], universal_newlines=True)\n",
    "                \n",
    "                match = version_pattern.search(version_output)\n",
    "                if match:\n",
    "                    return int(match.group(1))\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "            continue\n",
    "\n",
    "    # Fallback method for some systems\n",
    "    try:\n",
    "        # Try using Chrome itself to get version\n",
    "        version_output = subprocess.check_output(['chrome', '--version'], universal_newlines=True)\n",
    "        match = version_pattern.search(version_output)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        pass\n",
    "\n",
    "    # If all methods fail, return a default or None\n",
    "    print(\"Warning: Could not automatically detect Chrome version. Defaulting to version 108.\")\n",
    "    return 108  # Default fallback version\n",
    "\n",
    "class PatentFamilyScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        \"\"\"Initialize the scraper with enhanced compatibility options.\"\"\"\n",
    "        \n",
    "        # Detect Chrome version\n",
    "        chrome_version = get_chrome_version()\n",
    "        \n",
    "        options = uc.ChromeOptions()\n",
    "        \n",
    "        if headless:\n",
    "            options.add_argument('--headless')\n",
    "        \n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_argument('--disable-extensions')\n",
    "        \n",
    "        try:\n",
    "            self.driver = uc.Chrome(\n",
    "                options=options, \n",
    "                use_subprocess=True,  \n",
    "                version_main=chrome_version,  # Use detected version\n",
    "                suppress_welcome=True,\n",
    "                debug=False\n",
    "            )\n",
    "            \n",
    "            self.driver.set_page_load_timeout(30)\n",
    "            self.driver.set_window_size(1920, 1080)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize ChromeDriver with detected version {chrome_version}: {e}\")\n",
    "            print(\"Trying alternative initialization method...\")\n",
    "            \n",
    "            # Alternative initialization method with explicit version\n",
    "            self.driver = uc.Chrome(\n",
    "                options=options,\n",
    "                version_main=108,  # Explicit fallback version\n",
    "                driver_executable_path=None  \n",
    "            )\n",
    "\n",
    "    def add_random_delay(self, min_seconds=1, max_seconds=3):\n",
    "        \"\"\"Add a random delay to mimic human behavior.\"\"\"\n",
    "        time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "    def get_page_html(self, url):\n",
    "        \"\"\"\n",
    "        Navigate to the given URL and return the page HTML with robust error handling.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The URL to navigate to\n",
    "        \n",
    "        Returns:\n",
    "            str or None: Page source HTML if successful, None otherwise\n",
    "        \"\"\"\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"Attempt {attempt + 1}: Navigating to: {url}\")\n",
    "                \n",
    "                # Clear any existing cookies or local storage\n",
    "                self.driver.delete_all_cookies()\n",
    "                \n",
    "                # Navigate to the URL\n",
    "                self.driver.get(url)\n",
    "\n",
    "                # Wait for body element to ensure page is loaded\n",
    "                WebDriverWait(self.driver, 30).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "                )\n",
    "\n",
    "                # Add random delay to mimic human behavior\n",
    "                self.add_random_delay(3, 5)\n",
    "\n",
    "                # Return the page source\n",
    "                return self.driver.page_source\n",
    "\n",
    "            except TimeoutException:\n",
    "                print(f\"Timeout on attempt {attempt + 1}. Retrying...\")\n",
    "                continue\n",
    "            \n",
    "            except WebDriverException as e:\n",
    "                print(f\"WebDriver error on attempt {attempt + 1}: {e}\")\n",
    "                \n",
    "                # Add a longer delay between retries\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error on attempt {attempt + 1}: {e}\")\n",
    "                break\n",
    "\n",
    "        print(f\"Failed to retrieve HTML for {url} after {max_retries} attempts.\")\n",
    "        return None\n",
    "\n",
    "    def parse_html(self, html):\n",
    "        \"\"\"\n",
    "        Parse the HTML and extract family member information.\n",
    "        \n",
    "        Args:\n",
    "            html (str): HTML page source to parse\n",
    "        \n",
    "        Returns:\n",
    "            list: List of extracted family member details\n",
    "        \"\"\"\n",
    "        if not html:\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Look for the element containing \"Publié en tant que\" or \"Published as\"\n",
    "        published_as_element = soup.find(\n",
    "            lambda tag: tag.name == \"h5\" and \n",
    "            (\"Publié en tant que\" in tag.text or \"Published as\" in tag.text)\n",
    "        )\n",
    "        \n",
    "        if published_as_element:\n",
    "            # Get the next sibling span that contains the relevant content\n",
    "            content_element = published_as_element.find_next_sibling(\"span\")\n",
    "            if content_element:\n",
    "                # Extract all span elements within the content\n",
    "                spans = content_element.find_all('span')\n",
    "                return [span.get_text(strip=True) for span in spans]\n",
    "        \n",
    "        return []\n",
    "\n",
    "    def process_patent(self, row):\n",
    "        \"\"\"\n",
    "        Process a single patent row to extract family members.\n",
    "        \n",
    "        Args:\n",
    "            row (pandas.Series): Row from the patent DataFrame\n",
    "        \n",
    "        Returns:\n",
    "            dict: Processed patent information\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Construct URL for patent family\n",
    "            url = f\"https://worldwide.espacenet.com/patent/search/family/{row['Family number']}/publication/{row['first publication number']}?q=hydrogen%20battery\"\n",
    "\n",
    "            # Get page HTML\n",
    "            html = self.get_page_html(url)\n",
    "            \n",
    "            if html:\n",
    "                print(f\"Page HTML retrieved successfully for {row['first publication number']}.\")\n",
    "                \n",
    "                # Parse HTML to extract family members\n",
    "                family_members = self.parse_html(html)\n",
    "                \n",
    "                return {\n",
    "                    'index': row.name,  # Use the DataFrame index\n",
    "                    'family_members': family_members\n",
    "                }\n",
    "            else:\n",
    "                print(f\"Failed to retrieve the page HTML for {row['first publication number']}.\")\n",
    "                return {\n",
    "                    'index': row.name,\n",
    "                    'family_members': None\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['first publication number']}: {e}\")\n",
    "            return {\n",
    "                'index': row.name,\n",
    "                'family_members': None\n",
    "            }\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the browser when done.\"\"\"\n",
    "        if self.driver:\n",
    "            try:\n",
    "                self.driver.quit()\n",
    "            except Exception as e:\n",
    "                print(f\"Error closing driver: {e}\")\n",
    "\n",
    "def parallel_extract_family_members(df, max_workers=None):\n",
    "    \"\"\"\n",
    "    Extract family members in parallel using multiple worker threads.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing patent information\n",
    "        max_workers (int, optional): Maximum number of concurrent threads\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Updated DataFrame with family members\n",
    "    \"\"\"\n",
    "    # Use CPU count if not specified, with a minimum of 1 and maximum of 10\n",
    "    if max_workers is None:\n",
    "        max_workers = min(max(1, os.cpu_count() or 1), 10)\n",
    "    \n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Initialize the family_members column if not exists\n",
    "    if 'family_members' not in df_copy.columns:\n",
    "        df_copy['family_members'] = None\n",
    "    \n",
    "    # Create a scraper instance for getting the base URL setup\n",
    "    base_scraper = PatentFamilyScraper()\n",
    "    \n",
    "    try:\n",
    "        # Use ThreadPoolExecutor for I/O-bound tasks like web scraping\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit tasks for each row\n",
    "            futures = [executor.submit(base_scraper.process_patent, row) for _, row in df_copy.iterrows()]\n",
    "            \n",
    "            # Collect results\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result['family_members'] is not None:\n",
    "                    df_copy.at[result['index'], 'family_members'] = result['family_members']\n",
    "    \n",
    "    finally:\n",
    "        # Ensure the base scraper is closed\n",
    "        base_scraper.close()\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == '__main__':\n",
    "#     # Load your DataFrame here\n",
    "#     # df = pd.read_csv('your_patent_data.csv')\n",
    "    \n",
    "#     # Perform parallel extraction\n",
    "#     updated_df = parallel_extract_family_members(df)\n",
    "    \n",
    "#     # Display the results\n",
    "#     print(updated_df[['first publication number', 'family_members']].head())\n",
    "    \n",
    "#     # Optional: Save to CSV\n",
    "#     updated_df.to_csv('patent_family_members.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame:\n",
      "  Family number first publication number\n",
      "0       EP12345                EP2500001\n",
      "1       US67890             US2023123456\n",
      "2       JP54321             JP2022098765\n",
      "\n",
      "\n",
      "Navigating to: https://worldwide.espacenet.com/patent/search/family/EP12345/publication/EP2500001?q=hydrogen%20battery\n",
      "Navigating to: https://worldwide.espacenet.com/patent/search/family/US67890/publication/US2023123456?q=hydrogen%20battery\n",
      "Navigating to: https://worldwide.espacenet.com/patent/search/family/JP54321/publication/JP2022098765?q=hydrogen%20battery\n",
      "Page HTML retrieved successfully for EP2500001.\n",
      "Page HTML retrieved successfully for JP2022098765.\n",
      "Page HTML retrieved successfully for US2023123456.\n",
      "Updated DataFrame:\n",
      "  Family number first publication number family_members\n",
      "0       EP12345                EP2500001             []\n",
      "1       US67890             US2023123456             []\n",
      "2       JP54321             JP2022098765             []\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import os\n",
    "import subprocess\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import re\n",
    "\n",
    "class PatentFamilyScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        \"\"\"Initialize the scraper with enhanced compatibility options.\"\"\"\n",
    "        \n",
    "        options = uc.ChromeOptions()\n",
    "        \n",
    "        if headless:\n",
    "            options.add_argument('--headless')\n",
    "        \n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_argument('--disable-extensions')\n",
    "        \n",
    "        try:\n",
    "            self.driver = uc.Chrome(\n",
    "                options=options, \n",
    "                use_subprocess=True,  \n",
    "                version_main=None,  \n",
    "                suppress_welcome=True,\n",
    "                debug=False\n",
    "            )\n",
    "            \n",
    "            self.driver.set_page_load_timeout(30)\n",
    "            self.driver.set_window_size(1920, 1080)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize ChromeDriver: {e}\")\n",
    "            print(\"Trying alternative initialization method...\")\n",
    "            \n",
    "            # Alternative initialization method\n",
    "            self.driver = uc.Chrome(\n",
    "                options=options,\n",
    "                driver_executable_path=None  \n",
    "            )\n",
    "\n",
    "    def add_random_delay(self, min_seconds=1, max_seconds=3):\n",
    "        \"\"\"Add a random delay to mimic human behavior.\"\"\"\n",
    "        time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "    def get_page_html(self, url):\n",
    "        \"\"\"Navigate to the given URL and return the page HTML.\"\"\"\n",
    "        try:\n",
    "            print(f\"Navigating to: {url}\")\n",
    "            self.driver.get(url)\n",
    "\n",
    "            # Wait for body element to ensure page is loaded\n",
    "            WebDriverWait(self.driver, 20).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "\n",
    "            # Add random delay to mimic human behavior\n",
    "            self.add_random_delay(3, 5)\n",
    "\n",
    "            return self.driver.page_source\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\"Timed out waiting for the page to load.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse_html(self, html):\n",
    "        \"\"\"Parse the HTML and extract all span elements inside the 'Published as' content.\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Look for the element containing \"Publié en tant que\" or \"Published as\"\n",
    "        published_as_element = soup.find(lambda tag: tag.name == \"h5\" and (\"Publié en tant que\" in tag.text or \"Published as\" in tag.text))\n",
    "        \n",
    "        if published_as_element:\n",
    "            # Get the next sibling span that contains the relevant content\n",
    "            content_element = published_as_element.find_next_sibling(\"span\")\n",
    "            if content_element:\n",
    "                # Extract all span elements within the content\n",
    "                spans = content_element.find_all('span')\n",
    "                return [span.get_text(strip=True) for span in spans]\n",
    "        return []\n",
    "\n",
    "    def process_patent(self, row):\n",
    "        \"\"\"Process a single patent row to extract family members.\"\"\"\n",
    "        try:\n",
    "            # Construct URL for patent family\n",
    "            url = f\"https://worldwide.espacenet.com/patent/search/family/{row['Family number']}/publication/{row['first publication number']}?q=hydrogen%20battery\"\n",
    "\n",
    "            # Get page HTML\n",
    "            html = self.get_page_html(url)\n",
    "            \n",
    "            if html:\n",
    "                print(f\"Page HTML retrieved successfully for {row['first publication number']}.\")\n",
    "                \n",
    "                # Parse HTML to extract family members\n",
    "                family_members = self.parse_html(html)\n",
    "                \n",
    "                return {\n",
    "                    'index': row.name,  # Use the DataFrame index\n",
    "                    'first publication number': row['first publication number'],\n",
    "                    'family_members': family_members\n",
    "                }\n",
    "            else:\n",
    "                print(f\"Failed to retrieve the page HTML for {row['first publication number']}.\")\n",
    "                return {\n",
    "                    'index': row.name,\n",
    "                    'first publication number': row['first publication number'],\n",
    "                    'family_members': None\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['first publication number']}: {e}\")\n",
    "            return {\n",
    "                'index': row.name,\n",
    "                'first publication number': row['first publication number'],\n",
    "                'family_members': None\n",
    "            }\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the browser when done.\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "def parallel_extract_family_members(df, max_workers=None):\n",
    "    \"\"\"\n",
    "    Extract family members in parallel using multiple worker threads.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing patent information\n",
    "        max_workers (int, optional): Maximum number of concurrent threads\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Updated DataFrame with family members\n",
    "    \"\"\"\n",
    "    # Use CPU count if not specified, with a minimum of 1 and maximum of 10\n",
    "    if max_workers is None:\n",
    "        max_workers = min(max(1, os.cpu_count() or 1), 10)\n",
    "    \n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Initialize the family_members column if not exists\n",
    "    if 'family_members' not in df_copy.columns:\n",
    "        df_copy['family_members'] = None\n",
    "    \n",
    "    # Results list to collect processed data\n",
    "    results = []\n",
    "\n",
    "    # Use a single scraper for the entire process to manage resources\n",
    "    scraper = PatentFamilyScraper(headless=False)  # Set to False to see browser activity\n",
    "    \n",
    "    try:\n",
    "        # Use ThreadPoolExecutor for I/O-bound tasks like web scraping\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit tasks for each row\n",
    "            futures = [executor.submit(scraper.process_patent, row) for _, row in df_copy.iterrows()]\n",
    "            \n",
    "            # Collect results\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                \n",
    "                # Update DataFrame if family members found\n",
    "                if result['family_members'] is not None:\n",
    "                    df_copy.at[result['index'], 'family_members'] = result['family_members']\n",
    "    \n",
    "    finally:\n",
    "        # Ensure the scraper is closed\n",
    "        scraper.close()\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    # Create a sample DataFrame with patent data\n",
    "    df = pd.DataFrame({\n",
    "        'Family number': ['EP12345', 'US67890', 'JP54321'],\n",
    "        'first publication number': ['EP2500001', 'US2023123456', 'JP2022098765']\n",
    "    })\n",
    "\n",
    "    print(\"Initial DataFrame:\")\n",
    "    print(df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Perform parallel extraction\n",
    "    try:\n",
    "        updated_df = parallel_extract_family_members(df)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"Updated DataFrame:\")\n",
    "        print(updated_df)\n",
    "        \n",
    "        # Save to CSV\n",
    "        updated_df.to_csv('patent_family_members.csv', index=False)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame:\n",
      "  Family number first publication number\n",
      "0      37397092           US2006250902A1\n",
      "1      89124565           KR20230163874A\n",
      "2      85796535            KR102511391B1\n",
      "\n",
      "\n",
      "Navigating to: https://worldwide.espacenet.com/patent/search/family/37397092/publication/US2006250902A1?q=hydrogen%20battery\n",
      "Navigating to: https://worldwide.espacenet.com/patent/search/family/89124565/publication/KR20230163874A?q=hydrogen%20battery\n",
      "Navigating to: https://worldwide.espacenet.com/patent/search/family/85796535/publication/KR102511391B1?q=hydrogen%20battery\n",
      "Could not find specific patent family container. Continuing...\n",
      "Could not find specific patent family container. Continuing...\n",
      "Could not find specific patent family container. Continuing...\n",
      "Family members for US2006250902A1: ['KR102511391B1']\n",
      "Family members for KR20230163874A: ['KR102511391B1']\n",
      "Family members for KR102511391B1: ['KR102511391B1']\n",
      "Updated DataFrame:\n",
      "  Family number first publication number   family_members\n",
      "0      37397092           US2006250902A1  [KR102511391B1]\n",
      "1      89124565           KR20230163874A  [KR102511391B1]\n",
      "2      85796535            KR102511391B1  [KR102511391B1]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import os\n",
    "import subprocess\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import re\n",
    "\n",
    "class PatentFamilyScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        \"\"\"Initialize the scraper with enhanced compatibility options.\"\"\"\n",
    "        \n",
    "        options = uc.ChromeOptions()\n",
    "        \n",
    "        if headless:\n",
    "            options.add_argument('--headless')\n",
    "        \n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_argument('--disable-extensions')\n",
    "        \n",
    "        try:\n",
    "            self.driver = uc.Chrome(\n",
    "                options=options, \n",
    "                use_subprocess=True,  \n",
    "                version_main=None,  \n",
    "                suppress_welcome=True,\n",
    "                debug=False\n",
    "            )\n",
    "            \n",
    "            self.driver.set_page_load_timeout(30)\n",
    "            self.driver.set_window_size(1920, 1080)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize ChromeDriver: {e}\")\n",
    "            print(\"Trying alternative initialization method...\")\n",
    "            \n",
    "            # Alternative initialization method\n",
    "            self.driver = uc.Chrome(\n",
    "                options=options,\n",
    "                driver_executable_path=None  \n",
    "            )\n",
    "\n",
    "    def add_random_delay(self, min_seconds=1, max_seconds=3):\n",
    "        \"\"\"Add a random delay to mimic human behavior.\"\"\"\n",
    "        time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "    def get_page_html(self, url):\n",
    "        \"\"\"Navigate to the given URL and return the page HTML.\"\"\"\n",
    "        try:\n",
    "            print(f\"Navigating to: {url}\")\n",
    "            self.driver.get(url)\n",
    "\n",
    "            # Wait for multiple potential indicators of page load\n",
    "            wait = WebDriverWait(self.driver, 30)\n",
    "            \n",
    "            # Wait for body to be present\n",
    "            wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "            \n",
    "            # Wait for any potential dynamic content containers\n",
    "            try:\n",
    "                # Wait for a container that might hold patent family information\n",
    "                wait.until(\n",
    "                    EC.presence_of_element_located((By.XPATH, \"//div[contains(@class, 'patent-family') or contains(@class, 'publication-details')]\"))\n",
    "                )\n",
    "            except TimeoutException:\n",
    "                print(\"Could not find specific patent family container. Continuing...\")\n",
    "\n",
    "            # Additional wait to ensure JavaScript content is loaded\n",
    "            self.add_random_delay(5, 7)\n",
    "\n",
    "            # Scroll to trigger lazy loading if applicable\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            self.add_random_delay(2, 3)\n",
    "            self.driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "\n",
    "            return self.driver.page_source\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\"Timed out waiting for the page to load.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse_html(self, html):\n",
    "        \"\"\"Parse the HTML and extract all span elements inside the 'Published as' content.\"\"\"\n",
    "        if not html:\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Multiple strategies to find family member information\n",
    "        strategies = [\n",
    "            # Strategy 1: Look for \"Published as\" section\n",
    "            lambda: soup.find(lambda tag: tag.name == \"h5\" and (\"Publié en tant que\" in tag.text or \"Published as\" in tag.text)),\n",
    "            \n",
    "            # Strategy 2: Look for common div/span classes that might contain family info\n",
    "            lambda: soup.find('div', class_=re.compile(r'family|publication|patent')),\n",
    "            \n",
    "            # Strategy 3: Find any span with text related to family members\n",
    "            lambda: soup.find('span', string=re.compile(r'Family|Publication|Member'))\n",
    "        ]\n",
    "\n",
    "        # Try each strategy\n",
    "        for strategy in strategies:\n",
    "            element = strategy()\n",
    "            if element:\n",
    "                # If found through first strategy (Published as)\n",
    "                if element.name == \"h5\":\n",
    "                    content_element = element.find_next_sibling(\"span\")\n",
    "                    if content_element:\n",
    "                        spans = content_element.find_all('span')\n",
    "                        family_members = [span.get_text(strip=True) for span in spans]\n",
    "                        if family_members:\n",
    "                            return family_members\n",
    "\n",
    "                # If found through other strategies\n",
    "                spans = element.find_all('span')\n",
    "                family_members = [span.get_text(strip=True) for span in spans if span.get_text(strip=True)]\n",
    "                if family_members:\n",
    "                    return family_members\n",
    "\n",
    "        return []\n",
    "\n",
    "    def process_patent(self, row):\n",
    "        \"\"\"Process a single patent row to extract family members.\"\"\"\n",
    "        try:\n",
    "            # Construct URL for patent family\n",
    "            url = f\"https://worldwide.espacenet.com/patent/search/family/{row['Family number']}/publication/{row['first publication number']}?q=hydrogen%20battery\"\n",
    "\n",
    "            # Get page HTML\n",
    "            html = self.get_page_html(url)\n",
    "            \n",
    "            if html:\n",
    "                # Parse HTML to extract family members\n",
    "                family_members = self.parse_html(html)\n",
    "                \n",
    "                print(f\"Family members for {row['first publication number']}: {family_members}\")\n",
    "                \n",
    "                return {\n",
    "                    'index': row.name,  # Use the DataFrame index\n",
    "                    'first publication number': row['first publication number'],\n",
    "                    'family_members': family_members\n",
    "                }\n",
    "            else:\n",
    "                print(f\"Failed to retrieve the page HTML for {row['first publication number']}.\")\n",
    "                return {\n",
    "                    'index': row.name,\n",
    "                    'first publication number': row['first publication number'],\n",
    "                    'family_members': []\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['first publication number']}: {e}\")\n",
    "            return {\n",
    "                'index': row.name,\n",
    "                'first publication number': row['first publication number'],\n",
    "                'family_members': []\n",
    "            }\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the browser when done.\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "def parallel_extract_family_members(df, max_workers=None):\n",
    "    \"\"\"\n",
    "    Extract family members in parallel using multiple worker threads.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing patent information\n",
    "        max_workers (int, optional): Maximum number of concurrent threads\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Updated DataFrame with family members\n",
    "    \"\"\"\n",
    "    # Use CPU count if not specified, with a minimum of 1 and maximum of 10\n",
    "    if max_workers is None:\n",
    "        max_workers = min(max(1, os.cpu_count() or 1), 10)\n",
    "    \n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Initialize the family_members column if not exists\n",
    "    if 'family_members' not in df_copy.columns:\n",
    "        df_copy['family_members'] = None\n",
    "    \n",
    "    # Use a single scraper for the entire process to manage resources\n",
    "    scraper = PatentFamilyScraper(headless=False)  # Set to False to see browser activity\n",
    "    \n",
    "    try:\n",
    "        # Use ThreadPoolExecutor for I/O-bound tasks like web scraping\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit tasks for each row\n",
    "            futures = [executor.submit(scraper.process_patent, row) for _, row in df_copy.iterrows()]\n",
    "            \n",
    "            # Collect results\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                result = future.result()\n",
    "                # Update DataFrame with results\n",
    "                df_copy.at[result['index'], 'family_members'] = result['family_members']\n",
    "    \n",
    "    finally:\n",
    "        # Ensure the scraper is closed\n",
    "        scraper.close()\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    # Create a sample DataFrame with patent data\n",
    "    df = pd.DataFrame({\n",
    "        'Family number': ['37397092', '89124565', '85796535'],\n",
    "        'first publication number': ['US2006250902A1', 'KR20230163874A', 'KR102511391B1']\n",
    "    })\n",
    "\n",
    "    print(\"Initial DataFrame:\")\n",
    "    print(df.head(3))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Perform parallel extraction\n",
    "    try:\n",
    "        updated_df = parallel_extract_family_members(df)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"Updated DataFrame:\")\n",
    "        print(updated_df.head())\n",
    "        \n",
    "        # Save to CSV\n",
    "        #updated_df.to_csv('patent_family_members.csv', index=False)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame:\n",
      "  Family number first publication number\n",
      "0      37397092           US2006250902A1\n",
      "1      89124565           KR20230163874A\n",
      "2      85796535            KR102511391B1\n",
      "\n",
      "\n",
      "An error occurred during extraction: name 'scraper' is not defined\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import os\n",
    "import subprocess\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import re\n",
    "\n",
    "class PatentFamilyScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        \"\"\"Initialize the scraper with enhanced compatibility options.\"\"\"\n",
    "        \n",
    "        options = uc.ChromeOptions()\n",
    "        \n",
    "        if headless:\n",
    "            options.add_argument('--headless')\n",
    "        \n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_argument('--disable-extensions')\n",
    "        \n",
    "        try:\n",
    "            self.driver = uc.Chrome(\n",
    "                options=options, \n",
    "                use_subprocess=True,  \n",
    "                version_main=None,  \n",
    "                suppress_welcome=True,\n",
    "                debug=False\n",
    "            )\n",
    "            \n",
    "            self.driver.set_page_load_timeout(30)\n",
    "            self.driver.set_window_size(1920, 1080)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize ChromeDriver: {e}\")\n",
    "            print(\"Trying alternative initialization method...\")\n",
    "            \n",
    "            # Alternative initialization method\n",
    "            self.driver = uc.Chrome(\n",
    "                options=options,\n",
    "                driver_executable_path=None  \n",
    "            )\n",
    "\n",
    "    def add_random_delay(self, min_seconds=1, max_seconds=3):\n",
    "        \"\"\"Add a random delay to mimic human behavior.\"\"\"\n",
    "        time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "    def get_page_html(self, url):\n",
    "        \"\"\"Navigate to the given URL and return the page HTML.\"\"\"\n",
    "        try:\n",
    "            print(f\"Navigating to: {url}\")\n",
    "            self.driver.get(url)\n",
    "\n",
    "            # Wait for multiple potential indicators of page load\n",
    "            wait = WebDriverWait(self.driver, 30)\n",
    "            \n",
    "            # Wait for body to be present\n",
    "            wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "            \n",
    "            # Wait for any potential dynamic content containers\n",
    "            try:\n",
    "                # Wait for a container that might hold patent family information\n",
    "                wait.until(\n",
    "                    EC.presence_of_element_located((By.XPATH, \"//div[contains(@class, 'patent-family') or contains(@class, 'publication-details')]\"))\n",
    "                )\n",
    "            except TimeoutException:\n",
    "                print(\"Could not find specific patent family container. Continuing...\")\n",
    "\n",
    "            # Additional wait to ensure JavaScript content is loaded\n",
    "            self.add_random_delay(5, 7)\n",
    "\n",
    "            # Scroll to trigger lazy loading if applicable\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            self.add_random_delay(2, 3)\n",
    "            self.driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "\n",
    "            return self.driver.page_source\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\"Timed out waiting for the page to load.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse_html(self, html):\n",
    "        \"\"\"Parse the HTML and extract all span elements inside the 'Published as' content.\"\"\"\n",
    "        if not html:\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Multiple strategies to find family member information\n",
    "        strategies = [\n",
    "            # Strategy 1: Look for \"Published as\" section\n",
    "            lambda: soup.find(lambda tag: tag.name == \"h5\" and (\"Publié en tant que\" in tag.text or \"Published as\" in tag.text)),\n",
    "            \n",
    "            # Strategy 2: Look for common div/span classes that might contain family info\n",
    "            lambda: soup.find('div', class_=re.compile(r'family|publication|patent')),\n",
    "            \n",
    "            # Strategy 3: Find any span with text related to family members\n",
    "            lambda: soup.find('span', string=re.compile(r'Family|Publication|Member'))\n",
    "        ]\n",
    "\n",
    "        # Try each strategy\n",
    "        for strategy in strategies:\n",
    "            element = strategy()\n",
    "            if element:\n",
    "                # If found through first strategy (Published as)\n",
    "                if element.name == \"h5\":\n",
    "                    content_element = element.find_next_sibling(\"span\")\n",
    "                    if content_element:\n",
    "                        spans = content_element.find_all('span')\n",
    "                        family_members = [span.get_text(strip=True) for span in spans]\n",
    "                        if family_members:\n",
    "                            return family_members\n",
    "\n",
    "                # If found through other strategies\n",
    "                spans = element.find_all('span')\n",
    "                family_members = [span.get_text(strip=True) for span in spans if span.get_text(strip=True)]\n",
    "                if family_members:\n",
    "                    return family_members\n",
    "\n",
    "        return []\n",
    "\n",
    "    def process_patent_staic(self, row):\n",
    "        \"\"\"Process a single patent row to extract family members.\"\"\"\n",
    "        scraper = PatentFamilyScraper(headless=False)\n",
    "        try:\n",
    "            # Construct URL for patent family\n",
    "            url = f\"https://worldwide.espacenet.com/patent/search/family/{row['Family number']}/publication/{row['first publication number']}?q=hydrogen%20battery\"\n",
    "\n",
    "            # Get page HTML\n",
    "            html = self.get_page_html(url)\n",
    "            \n",
    "            if html:\n",
    "                # Parse HTML to extract family members\n",
    "                family_members = scraper.parse_html(html) if html else []\n",
    "                \n",
    "                print(f\"Family members for {row['first publication number']}: {family_members}\")\n",
    "                \n",
    "                return {\n",
    "                    'index': row.name,  # Use the DataFrame index\n",
    "                    'first publication number': row['first publication number'],\n",
    "                    'family_members': family_members\n",
    "                }\n",
    "            else:\n",
    "                print(f\"Failed to retrieve the page HTML for {row['first publication number']}.\")\n",
    "                return {\n",
    "                    'index': row.name,\n",
    "                    'first publication number': row['first publication number'],\n",
    "                    'family_members': []\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['first publication number']}: {e}\")\n",
    "            return {\n",
    "                'index': row.name,\n",
    "                'first publication number': row['first publication number'],\n",
    "                'family_members': []\n",
    "            }\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the browser when done.\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "def process_patent(self, row):\n",
    "        \"\"\"Process a single patent row to extract family members.\"\"\"\n",
    "        scraper = PatentFamilyScraper(headless=False)\n",
    "        try:\n",
    "            # Construct URL for patent family\n",
    "            url = f\"https://worldwide.espacenet.com/patent/search/family/{row['Family number']}/publication/{row['first publication number']}?q=hydrogen%20battery\"\n",
    "\n",
    "            # Get page HTML\n",
    "            html = self.get_page_html(url)\n",
    "            \n",
    "            if html:\n",
    "                # Parse HTML to extract family members\n",
    "                family_members = scraper.parse_html(html) if html else []\n",
    "                \n",
    "                print(f\"Family members for {row['first publication number']}: {family_members}\")\n",
    "                \n",
    "                return {\n",
    "                    'index': row.name,  # Use the DataFrame index\n",
    "                    'first publication number': row['first publication number'],\n",
    "                    'family_members': family_members\n",
    "                }\n",
    "            else:\n",
    "                print(f\"Failed to retrieve the page HTML for {row['first publication number']}.\")\n",
    "                return {\n",
    "                    'index': row.name,\n",
    "                    'first publication number': row['first publication number'],\n",
    "                    'family_members': []\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['first publication number']}: {e}\")\n",
    "            return {\n",
    "                'index': row.name,\n",
    "                'first publication number': row['first publication number'],\n",
    "                'family_members': []\n",
    "            }\n",
    "def parallel_extract_family_members(df, max_workers=None):\n",
    "    \"\"\"\n",
    "    Extract family members in parallel using multiple worker threads.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing patent information\n",
    "        max_workers (int, optional): Maximum number of concurrent threads\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Updated DataFrame with family members\n",
    "    \"\"\"\n",
    "    # Use CPU count if not specified, with a minimum of 1 and maximum of 10\n",
    "    if max_workers is None:\n",
    "        max_workers = min(max(1, os.cpu_count() or 1), 10)\n",
    "    \n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Initialize the family_members column if not exists\n",
    "    if 'family_members' not in df_copy.columns:\n",
    "        df_copy['family_members'] = None\n",
    "    \n",
    "    # Use a single scraper for the entire process to manage resources\n",
    "    #scraper = PatentFamilyScraper(headless=False)  # Set to False to see browser activity\n",
    "    \n",
    "    try:\n",
    "        # Use ThreadPoolExecutor for I/O-bound tasks like web scraping\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit tasks for each row\n",
    "            futures = [executor.submit(process_patent, row) for _, row in df_copy.iterrows()]\n",
    "            \n",
    "            # Collect results\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                result = future.result()\n",
    "                # Update DataFrame with results\n",
    "                df_copy.at[result['index'], 'family_members'] = result['family_members']\n",
    "    \n",
    "    finally:\n",
    "        # Ensure the scraper is closed\n",
    "        scraper.close()\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    # Create a sample DataFrame with patent data\n",
    "    df = pd.DataFrame({\n",
    "        'Family number': ['37397092', '89124565', '85796535'],\n",
    "        'first publication number': ['US2006250902A1', 'KR20230163874A', 'KR102511391B1']\n",
    "    })\n",
    "\n",
    "    print(\"Initial DataFrame:\")\n",
    "    print(df.head(3))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Perform parallel extraction\n",
    "    try:\n",
    "        updated_df = parallel_extract_family_members(df)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"Updated DataFrame:\")\n",
    "        print(updated_df.head())\n",
    "        \n",
    "        # Save to CSV\n",
    "        #updated_df.to_csv('patent_family_members.csv', index=False)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
